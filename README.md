
# üìò **Pipeline DECIDE (Classifica√ß√£o de Queries com LLMs)**

Este reposit√≥rio cont√©m a pipeline desenvolvida no √¢mbito do projeto **DECIDE**, com o objetivo de **replicar e estender** a metodologia descrita no estudo AIM (Artificial Intelligence Supported Development of Health Guidelines) para a an√°lise em larga escala de queries, recorrendo a **Large Language Models (LLMs)**.

A pipeline est√° organizada em **duas fases principais**:

* **Parte 1** ‚Äì Identifica√ß√£o de queries que transmitem uma pergunta expl√≠cita
* **Parte 2 + 3** ‚Äì Triagem ARIA e formula√ß√£o de perguntas de guideline em formato GRADE

---

## üìÇ **Estrutura do Projeto**

```
agreement/                      Valida√ß√£o e processamento agreement
archived_results/               Resultados de runs anteriores ou descartados
logs/                           Logs detalhados de execu√ß√£o

results_part1/                  Outputs finais da Parte 1
results_part2_3/                Outputs finais da Parte 2 + 3
xlsx_intermed/                  Ficheiros interm√©dios (debug, retries, merges)

pipeline_groupA_part1.py        Pipeline da Parte 1 (classifica√ß√£o de queries)
pipeline_groupA_part2_3.py      Pipeline da Parte 2 + 3 (ARIA + GRADE)

queries_middle_east.xlsx        Dataset de input inicial
README.md
.env
.gitignore
```
---

## üîπ Parte 1 ‚Äî Identifica√ß√£o de Queries com Pergunta Expl√≠cita

A Parte 1 tem como objetivo identificar se uma query **transmite explicitamente uma pergunta**, seguindo a metodologia descrita no **Supplement Box 2A e 2B** do artigo AIM.

### ‚úî Normaliza√ß√£o e deduplica√ß√£o

* Normaliza√ß√£o Unicode e limpeza de whitespace
* Remo√ß√£o de duplicados por texto
* Atribui√ß√£o de um `UniqueID` est√°vel a cada query √∫nica

---

### ‚úî Classifica√ß√£o baseada em LLMs

Cada query √∫nica √© classificada usando **tr√™s modelos**, com **duas runs independentes por modelo**:

* **Perplexity (sonar)**
* **OpenAI GPT-4o-mini**
* **Gemini 2.5 Flash**

Caracter√≠sticas principais:

* Processamento em **batches de 50 queries**
* Prompt baseado no **Supplement Box 2A**, adaptado e estendido
* Output **for√ßado a JSON**
* Parsing robusto para lidar com respostas parcialmente mal-formatadas
* Logs e exporta√ß√£o de respostas falhadas para ficheiros de debug

Para replicar fielmente o m√©todo do artigo:

> *‚ÄúDifferent query combinations were used in each round.‚Äù*

As queries s√£o:

* embaralhadas com `.sample(frac=1)`
* reagrupadas em batches diferentes em cada run

---

### ‚úî Classifica√ß√£o por regras lingu√≠sticas (determin√≠stica)

Em paralelo, √© aplicada uma classifica√ß√£o baseada em regras multilingues, inspirada no **Supplement Box 2B**, incluindo:

* palavras interrogativas (EN, ES, PT, FR, DE, NL, RU, AR, FA, TR)
* padr√µes sint√°ticos
* part√≠culas interrogativas
* dete√ß√£o de pedidos impl√≠citos

Prompt usado (**ChatGPT 5.1**):
* [https://chatgpt.com/share 6931b30c-4208-8004-9e29-98037d1dc763](https://chatgpt.com/share/6931b30c-4208-8004-9e29-98037d1dc763)

---

### ‚úî Merge final da Parte 1

Os resultados s√£o integrados usando o `UniqueID`, garantindo:

* consist√™ncia entre runs
* toler√¢ncia a pequenas varia√ß√µes de texto
* aus√™ncia de conflitos

O output final da Parte 1 completo √©:

```
LLM_complete_classification_PERP_GPT_GEM.xlsx
```

que cont√©m:

* Query
* UniqueID
* Classifica√ß√µes LLM (runs 1 e 2)
* Classifica√ß√£o por regras
* Colunas originais do dataset

O output final da Parte 1 apenas com queriess unicas √©:

```
LLM_class_unique_PERP_GPT_GEM.xlsx
```

---

## üîπ Parte 2 + 3 ‚Äî Triagem ARIA e Perguntas GRADE

A Parte 2 + 3 parte **exclusivamente do output da Parte 1**.

### Crit√©rio de elegibilidade

Uma query √© processada se **pelo menos um m√©todo da Parte 1** indicar que transmite uma pergunta expl√≠cita:

* `Rules == YES`
  **ou**
* qualquer coluna `LLM_run* == YES`

Este crit√©rio privilegia **sensibilidade m√°xima**.

---

### ‚úî Parte 2 ‚Äî Classifica√ß√£o ARIA

Cada query eleg√≠vel √© processada **independentemente** por:

* GPT-4o
* Perplexity Sonar-Pro
* Gemini-2.5-Pro

Cada modelo classifica a query como:

* **Unrelated**
* **Background**
* **Foreground**

Acompanhado de uma justifica√ß√£o textual para a sua classifica√ß√£o.

Os prompts utilizados correspondem integralmente aos prompts longos definidos a priori.

---

### ‚úî Parte 3 ‚Äî Formula√ß√£o de Perguntas GRADE

Para queries classificadas como **Foreground**, cada LLM gera **independentemente** uma pergunta estruturada no formato GRADE:

```
Should [Intervention] vs [Comparator] be used in [Population]?
```

N√£o √© aplicado qualquer mecanismo de consenso ou voting:

* cada LLM √© tratado como **pipeline anal√≠tico independente**
* diverg√™ncias s√£o consideradas objeto de an√°lise

Quando a interven√ß√£o √© demasiado vaga, o output √© explicitamente:

```
Error: Intervention too vague.
```

---

## üìä Outputs

* Resultados em formato **wide** (uma linha por `UniqueID`, colunas por modelo)
* Valores expl√≠citos `N/A` distinguem claramente:

  * queries n√£o processadas
  * queries n√£o aplic√°veis

Os resultados da Parte 2 + 3 s√£o posteriormente **integrados no dataset completo da Parte 1** atrav√©s de merge por `UniqueID`.

O resultado das queries classificadas na Parte 2 e 3:

```
PART2_3_queries_class.xlsx
```

O output final da Parte 2 e 3 para queries √∫nicas:

```
PART2_3_final_unique.xlsx
```

---

## ‚ö†Ô∏è Limita√ß√µes e Notas

### Robustez do parsing

* Outputs JSON podem falhar quando o modelo adiciona texto extra
* O c√≥digo inclui:

  * mecanismos de fallback
  * logging detalhado
  * exporta√ß√£o de respostas problem√°ticas
* A execu√ß√£o nunca √© interrompida por estas falhas

---

## üîÅ Reprodutibilidade

* A pipeline √© determin√≠stica dado o mesmo input e respostas das APIs
* Logs, ficheiros interm√©dios e resultados arquivados garantem rastreabilidade total
* O uso de `UniqueID` assegura consist√™ncia entre fases

---
