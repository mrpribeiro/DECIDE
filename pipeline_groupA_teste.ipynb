{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd727c1c",
   "metadata": {},
   "source": [
    "# DECIDE – Pipeline LLM para classificação de queries (Groq API)\n",
    "\n",
    "Este notebook replica o ficheiro `pipeline_groupA.py` mas em formato interativo,\n",
    "para podermos:\n",
    "- testar cada passo,\n",
    "- inspecionar DataFrames,\n",
    "- alterar parâmetros (modelo, batch size, prompt, etc.).\n",
    "\n",
    "### Ambiente recomendado\n",
    "\n",
    "```bash\n",
    "mamba create -n decide_env python=3.10\n",
    "mamba activate decide_env\n",
    "pip install groq pandas python-dotenv openpyxl perplexityai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from perplexity import Perplexity\n",
    "\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# ============================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "log_filename = datetime.now().strftime(\"pipeline_run_%Y-%m-%d_%H-%M-%S.log\")\n",
    "\n",
    "\n",
    "log_handler = RotatingFileHandler(\n",
    "    log_filename,\n",
    "    maxBytes=5_000_000,\n",
    "    backupCount=3,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "log_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger(\"DECIDE_PIPELINE\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURAR PERPLEXITY API\n",
    "# ============================================================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"PERPLEXITY_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Variável de ambiente PERPLEXITY_API_KEY não definida.\")\n",
    "\n",
    "client = Perplexity(api_key=api_key)\n",
    "\n",
    "MODEL_NAME = \"sonar\"   # ou \"sonar-pro\"\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "logger.info(\"Ambiente carregado e cliente Perplexity SDK configurado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bf893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALIZAÇÃO DE QUERIES\n",
    "# ============================================================\n",
    "\n",
    "def normalize_query(q):\n",
    "    \"\"\"Remove espaços invisíveis + normaliza Unicode + limpa whitespace.\"\"\"\n",
    "    if pd.isna(q):\n",
    "        return \"\"\n",
    "    q = str(q)\n",
    "\n",
    "    q = unicodedata.normalize(\"NFKC\", q)    # normalização Unicode\n",
    "    q = q.replace(\"\\u200b\", \"\")             # zero-width space\n",
    "    q = q.replace(\"\\xa0\", \" \")              # NBSP\n",
    "    q = \" \".join(q.split())                 # remover múltiplos espaços / trim\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. LER FICHEIRO EXCEL\n",
    "# ============================================================\n",
    "\n",
    "def load_queries(path=\"queries_middle_east_test.xlsx\"):\n",
    "    df = pd.read_excel(path)\n",
    "    # df = df.tail(5)  # PARA TESTES RÁPIDOS\n",
    "\n",
    "    if \"Query\" not in df.columns:\n",
    "        raise ValueError(\"A coluna 'Query' não existe no ficheiro.\")\n",
    "\n",
    "    # Remover linhas com queries vazias\n",
    "    df = df.dropna(subset=[\"Query\"]).copy()\n",
    "\n",
    "    # print('Antes')\n",
    "    # print(df[\"Query\"])\n",
    "    # print('---')\n",
    "    # Normalização das queries\n",
    "    df[\"Query\"] = df[\"Query\"].apply(normalize_query)\n",
    "    # print('Depois')\n",
    "    # print(df[\"Query\"])\n",
    "\n",
    "    # df[\"QueryID\"] = range(1, len(df) + 1)\n",
    "    return df\n",
    "\n",
    "# TESTE: carregar e espreitar\n",
    "df = load_queries()\n",
    "logger.info(f\"Total de linhas: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a84ef3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 10:20:04 | INFO | Queries únicas: 293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>UniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antihistaminique</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antihistaminiques</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مضادات الهيستامين</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>les antihistaminiques</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anti histaminique</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Query  UniqueID\n",
       "0       antihistaminique         1\n",
       "1      antihistaminiques         2\n",
       "2      مضادات الهيستامين         3\n",
       "3  les antihistaminiques         4\n",
       "4      anti histaminique         5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. REMOVER DUPLICADOS POR TEXTO\n",
    "# ============================================================\n",
    "\n",
    "def deduplicate_queries(df):\n",
    "    df_unique = df[[\"Query\"]].drop_duplicates().reset_index(drop=True)\n",
    "    df_unique[\"UniqueID\"] = range(1, len(df_unique) + 1)\n",
    "    return df_unique\n",
    "\n",
    "df_unique = deduplicate_queries(df)\n",
    "logger.info(f\"Queries únicas: {len(df_unique)}\")\n",
    "df_unique.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. BATCHING\n",
    "# ============================================================\n",
    "\n",
    "def chunk_list(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5dc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. PROMPT — SUPPLEMENT BOX 2A (COM JSON)\n",
    "# ============================================================\n",
    "\n",
    "def build_prompt_for_batch(batch):\n",
    "    text = (\n",
    "        \"We have extracted several queries from GoogleTrends.\\n\\n\"\n",
    "        \"We want to identify what are the queries which explicitly convey a question. \"\n",
    "        \"These queries can be in different languages, including English, Arabic, \"\n",
    "        \"French, Persian (Farsi), Turkish, Russian, Spanish, German or Dutch.\\n\\n\"\n",
    "        \"Below, you can find the list of queries.\\n\\n\"\n",
    "        \"Return ONLY a JSON array, with no explanations or additional text. \"\n",
    "        \"Each element must have the form:\\n\"\n",
    "        \"{ \\\"query\\\": \\\"<query text>\\\", \\\"explicit_question\\\": \\\"YES\\\" or \\\"NO\\\" }\\n\\n\"\n",
    "        \"Write \\\"YES\\\" only if the query explicitly conveys a question. \"\n",
    "        \"Otherwise, write \\\"NO\\\".\\n\\n\"\n",
    "        \"List of queries to be classified:\\n\"\n",
    "    )\n",
    "\n",
    "    for q in batch:\n",
    "        text += f\"- {q}\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# ============================================================\n",
    "# 5. FUNÇÃO PARA EXTRAIR JSON DA RESPOSTA DO LLM\n",
    "# ============================================================\n",
    "\n",
    "def safe_json_extract(text):\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    cleaned = text.strip()\n",
    "\n",
    "    # 1) Strip leading markdown fence like ```json\n",
    "    if cleaned.startswith(\"```\"):\n",
    "        cleaned = re.sub(r\"^```[a-zA-Z]*\\s*\", \"\", cleaned)\n",
    "\n",
    "    # 2) Strip trailing fence ```\n",
    "    if cleaned.endswith(\"```\"):\n",
    "        cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)\n",
    "\n",
    "    # 3) Extract the first JSON array\n",
    "    match = re.search(r\"\\[.*\\]\", cleaned, re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    candidate = match.group(0).strip()\n",
    "\n",
    "    try:\n",
    "        return json.loads(candidate)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b789a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have extracted several queries from GoogleTrends from the following nine countries: Algeria, Egypt, Iran, Iraq, Morocco, Pakistan, Saudi Arabia, Turkey and the United Arab Emirates.\n",
      "\n",
      "We want to identify which queries explicitly convey a question. These queries can be in English, Portuguese, French, Spanish or in any language spoken in the aforementioned countries.\n",
      "\n",
      "Below, you can find the list of queries.\n",
      "\n",
      "For each query, return a JSON array where each element has the form:\n",
      "{ \"query\": \"<query text>\", \"explicit_question\": \"YES\" or \"NO\" }\n",
      "\n",
      "Write \"YES\" only if the query explicitly conveys a question. Otherwise, write \"NO\".\n",
      "\n",
      "List of queries to be classified:\n",
      "- antihistaminique\n",
      "- antihistaminiques\n",
      "- مضادات الهيستامين\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prompt = build_prompt_for_batch(df_unique[\"Query\"].head(3).tolist())\n",
    "print(test_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca6591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'antihistaminique', 'explicit_question': 'NO'},\n",
       " {'query': 'antihistaminiques', 'explicit_question': 'NO'},\n",
       " {'query': 'مضادات الهيستامين', 'explicit_question': 'NO'},\n",
       " {'query': 'les antihistaminiques', 'explicit_question': 'NO'},\n",
       " {'query': 'anti histaminique', 'explicit_question': 'NO'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. FUNÇÃO PARA CLASSIFICAR UM BATCH COM O LLM\n",
    "# ============================================================\n",
    "def classify_batch_with_llm(batch):\n",
    "    \"\"\"\n",
    "    Devolve lista de dicts: { \"query\": ..., \"explicit_question\": \"YES\"/\"NO\" }\n",
    "    \"\"\"\n",
    "    prompt = build_prompt_for_batch(batch)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0  # respostas mais determinísticas, menos aleatórias\n",
    "    )\n",
    "\n",
    "    raw = response.choices[0].message.content\n",
    "    # print(raw)  # para debug\n",
    "\n",
    "    data = safe_json_extract(raw)\n",
    "    if data is None:\n",
    "        logger.error(\"Falha ao extrair JSON; a guardar resposta bruta para debug.\")\n",
    "        with open(\"failed_batch.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(raw + \"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        return []\n",
    "\n",
    "    cleaned = []\n",
    "    for item in data:\n",
    "        cleaned.append({\n",
    "            \"query\": item.get(\"query\", \"\").strip(),\n",
    "            \"explicit_question\": item.get(\"explicit_question\", \"NO\").strip().upper()\n",
    "        })\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# TESTE COM UM MINI-BATCH\n",
    "mini_batch = df_unique[\"Query\"].head(5).tolist()\n",
    "test_output = classify_batch_with_llm(mini_batch)\n",
    "test_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6cacfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 10:20:49 | INFO | LLM_run1 - batch com 20 queries...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>LLM_run1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UniqueID LLM_run1\n",
       "0         1       NO\n",
       "1         2       NO\n",
       "2         3       NO\n",
       "3         4       NO\n",
       "4         5       NO"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. FUNÇÃO: EXECUTAR UMA RUN COMPLETA (RUN 1 / RUN 2)\n",
    "# ============================================================\n",
    "\n",
    "def run_llm_classification(df_unique, run_name):\n",
    "    rows = df_unique[[\"UniqueID\", \"Query\"]]\n",
    "    results = []\n",
    "\n",
    "    for batch_df in chunk_list(rows, BATCH_SIZE):\n",
    "        logger.info(f\"{run_name} - batch com {len(batch_df)} queries...\")\n",
    "\n",
    "        batch_queries = batch_df[\"Query\"].tolist()\n",
    "        batch_ids = batch_df[\"UniqueID\"].tolist()\n",
    "\n",
    "        out = classify_batch_with_llm(batch_queries)\n",
    "\n",
    "        for uid, item in zip(batch_ids, out):\n",
    "            results.append({\n",
    "                \"UniqueID\": uid,\n",
    "                run_name: item[\"explicit_question\"]\n",
    "            })\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ⚠️ Para testes usa um subset:\n",
    "df_unique_test = df_unique.head(20).copy()\n",
    "df_run1_test = run_llm_classification(df_unique_test, \"LLM_run1\")\n",
    "df_run1_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39381f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>Rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antihistaminique</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antihistaminiques</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مضادات الهيستامين</td>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>les antihistaminiques</td>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anti histaminique</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Query  UniqueID Rules\n",
       "0       antihistaminique         1    NO\n",
       "1      antihistaminiques         2    NO\n",
       "2      مضادات الهيستامين         3    NO\n",
       "3  les antihistaminiques         4    NO\n",
       "4      anti histaminique         5    NO"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. CLASSIFICAÇÃO POR REGRAS MULTILINGUE\n",
    "# ============================================================\n",
    "\n",
    "# 1) Interrogative keywords\n",
    "QUESTION_KEYWORDS = [\n",
    "    # English\n",
    "    \"what\", \"when\", \"where\", \"why\", \"how\", \"who\", \"whom\", \"which\",\n",
    "    \"do\", \"did\", \"does\", \"are\", \"is\", \"can\", \"could\", \"should\", \"would\",\n",
    "\n",
    "    # French\n",
    "    \"quoi\", \"quand\", \"où\", \"pourquoi\", \"comment\", \"qui\", \"lequel\",\n",
    "    \"est-ce\", \"peux-tu\", \"pourrais-tu\",\n",
    "\n",
    "    # Spanish\n",
    "    \"qué\", \"cuándo\", \"dónde\", \"por qué\", \"cómo\", \"quién\", \"cuál\", \"puedes\", \"podrías\",\n",
    "\n",
    "    # German\n",
    "    \"was\", \"wann\", \"wo\", \"warum\", \"wie\", \"wer\", \"welche\", \"kann\", \"könnte\",\n",
    "\n",
    "    # Dutch\n",
    "    \"wat\", \"wanneer\", \"waar\", \"waarom\", \"hoe\", \"wie\", \"welke\", \"kan\", \"zou\",\n",
    "\n",
    "    # Russian\n",
    "    \"что\", \"когда\", \"где\", \"почему\", \"как\", \"кто\", \"который\", \"может\", \"могли бы\",\n",
    "\n",
    "    # Arabic\n",
    "    \"ماذا\", \"متى\", \"أين\", \"لماذا\", \"كيف\", \"من\", \"هل\", \"أيمكن\",\n",
    "\n",
    "    # Persian\n",
    "    \"چه\", \"کی\", \"کجا\", \"چرا\", \"چطور\", \"کیست\", \"آیا\",\n",
    "\n",
    "    # Turkish\n",
    "    \"ne\", \"ne zaman\", \"nerede\", \"neden\", \"nasıl\", \"kim\", \"hangi\", \"mı\", \"mi\", \"mu\", \"mü\"\n",
    "]\n",
    "\n",
    "# 2) Structural patterns\n",
    "STRUCTURAL_PATTERNS = [\n",
    "    r\".*\\?\\s*$\",                                  # explicit '?'\n",
    "    r\"^(can|could|should|would|do|did|does)\\b\",   # English inversion\n",
    "    r\"^(is|are|was|were|am)\\b\",                   # English BE inversion\n",
    "    r\".*\\b(mı|mi|mu|mü)\\?$\",                      # Turkish question particle\n",
    "    r\"^[^.!?]*\\b(est-ce que)\\b\",                  # French\n",
    "    r\"^[^.!?]*\\b(هل)\\b\",                          # Arabic\n",
    "    r\"^[^.!?]*\\b(آیا)\\b\",                         # Persian\n",
    "]\n",
    "\n",
    "# 3) Implicit question markers\n",
    "IMPLICIT_PATTERNS = [\n",
    "    r\"could you\\b.*\",\n",
    "    r\"would you\\b.*\",\n",
    "    r\"can you\\b.*\",\n",
    "    r\"please explain\\b.*\",\n",
    "    r\"i wonder if\\b.*\",\n",
    "    r\"i would like to know\\b.*\"\n",
    "]\n",
    "\n",
    "\n",
    "def is_question_multilingual(sentence: str) -> bool:\n",
    "    if not sentence:\n",
    "        return False\n",
    "\n",
    "    s = sentence.strip().lower()\n",
    "\n",
    "    # Rule 1: punctuation\n",
    "    if re.search(r\".*\\?\\s*$\", s):\n",
    "        return True\n",
    "\n",
    "    # Rule 2: structural patterns\n",
    "    for p in STRUCTURAL_PATTERNS:\n",
    "        if re.search(p, s):\n",
    "            return True\n",
    "\n",
    "    # Rule 3: keyword-based detection\n",
    "    for kw in QUESTION_KEYWORDS:\n",
    "        if re.search(rf\"\\b{re.escape(kw)}\\b\", s):\n",
    "            return True\n",
    "\n",
    "    # Rule 4: implicit patterns\n",
    "    for p in IMPLICIT_PATTERNS:\n",
    "        if re.search(p, s):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def apply_multilingual_rules(df_unique):\n",
    "    df_unique[\"Rules\"] = df_unique[\"Query\"].apply(\n",
    "        lambda x: \"YES\" if is_question_multilingual(x) else \"NO\"\n",
    "    )\n",
    "    return df_unique\n",
    "\n",
    "# TESTE\n",
    "df_unique_test = apply_multilingual_rules(df_unique.head(20).copy())\n",
    "df_unique_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3960ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. MERGE FINAL\n",
    "# ============================================================\n",
    "\n",
    "def merge_results(df_original, df_unique, df_run1, df_run2):\n",
    "    temp = df_unique.merge(df_run1, on=\"UniqueID\", how=\"left\")\n",
    "    temp = temp.merge(df_run2, on=\"UniqueID\", how=\"left\")\n",
    "\n",
    "    df_final = df_original.merge(\n",
    "        temp[[\"Query\", \"Rules\", \"LLM_run1\", \"LLM_run2\"]],\n",
    "        on=\"Query\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Exemplo de pipeline completo em modo “manual”:\n",
    "\n",
    "start_time = time.time()\n",
    "logger.info(\"=== PIPELINE COMPLETO (TESTE EM SUBSET) ===\")\n",
    "\n",
    "df = load_queries()\n",
    "df_unique = deduplicate_queries(df)\n",
    "\n",
    "df_unique.to_excel(\"unique_test.xlsx\", index=False)\n",
    "\n",
    "# Para testes, subset:\n",
    "df_unique_small = df_unique.tail(20).copy()\n",
    "\n",
    "df_run1 = run_llm_classification(df_unique, \"LLM_run1\")\n",
    "df_run1.to_excel(\"df_run1_test.xlsx\", index=False)\n",
    "\n",
    "df_unique_shuffled = df_unique.sample(frac=1, random_state=None).reset_index(drop=True)  # sample() função pandas usada para escolher linhas de forma aleatória\n",
    "df_run2 = run_llm_classification(df_unique_shuffled, \"LLM_run2\")\n",
    "# df_run2 = run_llm_classification(df_unique, \"LLM_run2\")\n",
    "df_run2.to_excel(\"df_run2_test.xlsx\", index=False)\n",
    "df_unique = apply_multilingual_rules(df_unique)\n",
    "df_unique.to_excel(\"df_unique_small_test.xlsx\", index=False)\n",
    "\n",
    "df_final_test = merge_results(df, df_unique, df_run1, df_run2)\n",
    "df_final_test.to_excel(\"df_final_test.xlsx\", index=False)\n",
    "df_final_test.head()\n",
    "\n",
    "elapsed = int(time.time() - start_time)\n",
    "logger.info(f\"Tempo total (subset): {elapsed} segundos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decide_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
